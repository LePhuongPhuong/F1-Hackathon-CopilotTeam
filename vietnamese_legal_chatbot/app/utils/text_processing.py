"""
Vietnamese Text Processing for Legal AI Chatbot
Xá»­ lÃ½ VÄƒn báº£n Tiáº¿ng Viá»‡t cho Chatbot AI PhÃ¡p lÃ½

Specialized text processing for Vietnamese legal documents and queries.
Xá»­ lÃ½ vÄƒn báº£n chuyÃªn biá»‡t cho tÃ i liá»‡u vÃ  truy váº¥n phÃ¡p lÃ½ tiáº¿ng Viá»‡t.
"""

import re
import unicodedata
from typing import List, Dict, Optional, Tuple, Set, Any
from dataclasses import dataclass
import logging

# TODO: Import khi implement
# import underthesea
# from pyvi import ViTokenizer
# from app.utils.config import settings

@dataclass
class VietnameseTextAnalysis:
    """Analysis result for Vietnamese text"""
    original_text: str
    normalized_text: str
    tokens: List[str]
    legal_terms: List[str]
    entities: List[Dict[str, str]]
    language_confidence: float

class VietnameseTextProcessor:
    """Main processor for Vietnamese text in legal context"""
    
    def __init__(self):
        """Initialize Vietnamese text processor"""
        self.legal_terms = self._load_legal_terms()
        self.stopwords = self._load_vietnamese_stopwords()
        self.legal_abbreviations = self._load_legal_abbreviations()
        
    def _load_legal_terms(self) -> List[str]:
        """Load Vietnamese legal terms dictionary"""
        return [
            # Quyá»n vÃ  nghÄ©a vá»¥
            "quyá»n dÃ¢n sá»±", "quyá»n con ngÆ°á»i", "quyá»n cÆ¡ báº£n", "nghÄ©a vá»¥ cÃ´ng dÃ¢n",
            "quyá»n sá»Ÿ há»¯u", "quyá»n sá»­ dá»¥ng", "quyá»n thá»«a káº¿", "quyá»n tÃ¡c giáº£",
            
            # CÆ¡ quan nhÃ  nÆ°á»›c
            "quá»‘c há»™i", "chÃ­nh phá»§", "thá»§ tÆ°á»›ng", "bá»™ trÆ°á»Ÿng", "tÃ²a Ã¡n",
            "viá»‡n kiá»ƒm sÃ¡t", "cÃ´ng an", "bá»™ tÆ° phÃ¡p", "cá»¥c phÃ¡p cháº¿",
            
            # VÄƒn báº£n phÃ¡p luáº­t
            "hiáº¿n phÃ¡p", "luáº­t", "bá»™ luáº­t", "nghá»‹ quyáº¿t", "nghá»‹ Ä‘á»‹nh",
            "thÃ´ng tÆ°", "quyáº¿t Ä‘á»‹nh", "chá»‰ thá»‹", "phÃ¡p lá»‡nh",
            
            # LÄ©nh vá»±c phÃ¡p lÃ½
            "dÃ¢n sá»±", "hÃ¬nh sá»±", "lao Ä‘á»™ng", "thÆ°Æ¡ng máº¡i", "hÃ nh chÃ­nh",
            "hÃ´n nhÃ¢n gia Ä‘Ã¬nh", "báº¥t Ä‘á»™ng sáº£n", "mÃ´i trÆ°á»ng", "thuáº¿",
            
            # Thá»§ tá»¥c phÃ¡p lÃ½
            "khá»Ÿi kiá»‡n", "tá»‘ tá»¥ng", "phÃºc tháº©m", "giÃ¡m Ä‘á»‘c tháº©m", "thi hÃ nh Ã¡n",
            "hÃ²a giáº£i", "trá»ng tÃ i", "bá»“i thÆ°á»ng", "xá»­ pháº¡t", "Ã¡n phÃ­",
            
            # Chá»§ thá»ƒ phÃ¡p lÃ½
            "cÃ´ng dÃ¢n", "phÃ¡p nhÃ¢n", "doanh nghiá»‡p", "tá»• chá»©c", "cÃ¡ nhÃ¢n",
            "ngÆ°á»i lao Ä‘á»™ng", "ngÆ°á»i sá»­ dá»¥ng lao Ä‘á»™ng", "Ä‘áº¡i diá»‡n phÃ¡p luáº­t"
        ]
    
    def _load_vietnamese_stopwords(self) -> Set[str]:
        """Load Vietnamese stopwords for legal text"""
        return {
            # Common Vietnamese stopwords
            "vÃ ", "cá»§a", "trong", "vá»›i", "vá»", "Ä‘á»ƒ", "cho", "tá»«", "theo",
            "nhÆ°", "khi", "náº¿u", "mÃ ", "nÃ y", "Ä‘Ã³", "cÃ¡c", "nhá»¯ng", "má»™t",
            "cÃ³", "lÃ ", "Ä‘Æ°á»£c", "bá»‹", "pháº£i", "cáº§n", "sáº½", "Ä‘Ã£", "Ä‘ang",
            "táº¡i", "trÃªn", "dÆ°á»›i", "giá»¯a", "ngoÃ i", "bÃªn", "sau", "trÆ°á»›c",
            
            # Legal-specific stopwords
            "quy Ä‘á»‹nh", "theo nhÆ°", "cÄƒn cá»©", "trÃªn cÆ¡ sá»Ÿ", "phÃ¹ há»£p",
            "tuÃ¢n thá»§", "thá»±c hiá»‡n", "Ã¡p dá»¥ng", "ban hÃ nh", "cÃ³ hiá»‡u lá»±c"
        }
    
    def _load_legal_abbreviations(self) -> Dict[str, str]:
        """Load Vietnamese legal abbreviations"""
        return {
            # Government agencies
            "BTP": "Bá»™ TÆ° phÃ¡p",
            "BCA": "Bá»™ CÃ´ng an", 
            "TANDTC": "TÃ²a Ã¡n nhÃ¢n dÃ¢n tá»‘i cao",
            "VKSTC": "Viá»‡n kiá»ƒm sÃ¡t nhÃ¢n dÃ¢n tá»‘i cao",
            
            # Legal documents
            "NÄ-CP": "Nghá»‹ Ä‘á»‹nh cá»§a ChÃ­nh phá»§",
            "QÄ-TTg": "Quyáº¿t Ä‘á»‹nh cá»§a Thá»§ tÆ°á»›ng",
            "TT-BTP": "ThÃ´ng tÆ° cá»§a Bá»™ TÆ° phÃ¡p",
            "CV": "CÃ´ng vÄƒn",
            
            # Legal codes
            "BLDS": "Bá»™ luáº­t DÃ¢n sá»±",
            "BLHS": "Bá»™ luáº­t HÃ¬nh sá»±", 
            "BLLD": "Bá»™ luáº­t Lao Ä‘á»™ng",
            "BLTM": "Bá»™ luáº­t ThÆ°Æ¡ng máº¡i",
            
            # Others
            "TP.HCM": "ThÃ nh phá»‘ Há»“ ChÃ­ Minh",
            "UBND": "á»¦y ban nhÃ¢n dÃ¢n",
            "HÄND": "Há»™i Ä‘á»“ng nhÃ¢n dÃ¢n"
        }
        
    def process_legal_text(self, text: str) -> VietnameseTextAnalysis:
        """Process Vietnamese legal text comprehensively"""
        try:
            # 1. Normalize text
            normalized = self.normalize_vietnamese_text(text)
            
            # 2. Tokenize
            tokens = self.tokenize_vietnamese(normalized)
            
            # 3. Extract legal terms
            legal_terms = self.extract_legal_terms(normalized)
            
            # 4. Extract entities
            entities = self.extract_legal_entities(normalized)
            
            # 5. Calculate confidence
            confidence = self._calculate_language_confidence(text)
            
            return VietnameseTextAnalysis(
                original_text=text,
                normalized_text=normalized,
                tokens=tokens,
                legal_terms=legal_terms,
                entities=entities,
                language_confidence=confidence
            )
            
        except Exception as e:
            logging.error(f"Vietnamese text processing failed: {e}")
            # Return basic analysis
            return VietnameseTextAnalysis(
                original_text=text,
                normalized_text=text,
                tokens=text.split(),
                legal_terms=[],
                entities=[],
                language_confidence=0.5
            )
    
    def normalize_vietnamese_text(self, text: str) -> str:
        """Normalize Vietnamese text for better processing"""
        try:
            # 1. Unicode normalization
            text = unicodedata.normalize('NFC', text)
            
            # 2. Clean whitespace
            text = re.sub(r'\s+', ' ', text)
            text = text.strip()
            
            # 3. Standardize punctuation
            text = re.sub(r'([.!?])\s*([.!?]+)', r'\1', text)
            
            # 4. Normalize Vietnamese legal terms
            text = self._normalize_legal_terms(text)
            
            # 5. Handle abbreviations
            text = self._expand_abbreviations(text)
            
            # 6. Standardize quotes
            text = re.sub(r'["""]', '"', text)
            text = re.sub(r"[''']", "'", text)
            
            return text
            
        except Exception as e:
            logging.error(f"Text normalization failed: {e}")
            return text
    
    def tokenize_vietnamese(self, text: str) -> List[str]:
        """Tokenize Vietnamese text using appropriate methods"""
        # TODO: Implement Vietnamese tokenization
        try:
            # Using underthesea or pyvi for better Vietnamese tokenization
            # tokens = underthesea.word_tokenize(text)
            
            # Fallback: simple tokenization
            tokens = text.split()
            
            # Filter out stopwords and clean tokens
            filtered_tokens = []
            for token in tokens:
                cleaned_token = self._clean_token(token)
                if cleaned_token and cleaned_token.lower() not in self.stopwords:
                    filtered_tokens.append(cleaned_token)
            
            return filtered_tokens
            
        except Exception as e:
            logging.error(f"Tokenization failed: {e}")
            return text.split()
    
    def extract_legal_terms(self, text: str) -> List[str]:
        """Extract Vietnamese legal terms from text"""
        try:
            found_terms = []
            text_lower = text.lower()
            
            # Search for legal terms in text
            for term in self.legal_terms:
                if term.lower() in text_lower:
                    found_terms.append(term)
            
            # Extract legal document references
            legal_refs = self._extract_legal_references(text)
            found_terms.extend(legal_refs)
            
            # Remove duplicates while preserving order
            seen = set()
            unique_terms = []
            for term in found_terms:
                if term not in seen:
                    seen.add(term)
                    unique_terms.append(term)
            
            return unique_terms
            
        except Exception as e:
            logging.error(f"Legal term extraction failed: {e}")
            return []
    
    def extract_legal_entities(self, text: str) -> List[Dict[str, str]]:
        """Extract legal entities from Vietnamese text"""
        entities = []
        
        try:
            # 1. Extract legal document references
            doc_refs = self._extract_document_references(text)
            entities.extend(doc_refs)
            
            # 2. Extract monetary amounts
            amounts = self._extract_monetary_amounts(text)
            entities.extend(amounts)
            
            # 3. Extract dates
            dates = self._extract_dates(text)
            entities.extend(dates)
            
            # 4. Extract person/organization names
            names = self._extract_names(text)
            entities.extend(names)
            
            # 5. Extract legal procedures
            procedures = self._extract_procedures(text)
            entities.extend(procedures)
            
            return entities
            
        except Exception as e:
            logging.error(f"Entity extraction failed: {e}")
            return []
    
    def _normalize_legal_terms(self, text: str) -> str:
        """Normalize Vietnamese legal terms"""
        # Standardize common legal term variations
        replacements = {
            'Bá»™ luáº­t dÃ¢n sá»±': 'Bá»™ luáº­t DÃ¢n sá»±',
            'bá»™ luáº­t dÃ¢n sá»±': 'Bá»™ luáº­t DÃ¢n sá»±',
            'Bá»™ luáº­t hÃ¬nh sá»±': 'Bá»™ luáº­t HÃ¬nh sá»±',
            'bá»™ luáº­t hÃ¬nh sá»±': 'Bá»™ luáº­t HÃ¬nh sá»±',
            'Bá»™ luáº­t lao Ä‘á»™ng': 'Bá»™ luáº­t Lao Ä‘á»™ng',
            'bá»™ luáº­t lao Ä‘á»™ng': 'Bá»™ luáº­t Lao Ä‘á»™ng',
        }
        
        for old, new in replacements.items():
            text = text.replace(old, new)
        
        return text
    
    def _expand_abbreviations(self, text: str) -> str:
        """Expand Vietnamese legal abbreviations"""
        for abbrev, full_form in self.legal_abbreviations.items():
            text = re.sub(r'\b' + re.escape(abbrev) + r'\b', full_form, text)
        
        return text
    
    def _clean_token(self, token: str) -> str:
        """Clean individual token"""
        # Remove punctuation from ends
        token = re.sub(r'^[^\w]+|[^\w]+$', '', token)
        
        # Keep only meaningful tokens
        if len(token) < 2:
            return ""
        
        return token
    
    def _extract_legal_references(self, text: str) -> List[str]:
        """Extract legal document references"""
        references = []
        
        # Pattern for Vietnamese legal references
        patterns = [
            r'(Äiá»u\s+\d+)',                          # Article references
            r'(Khoáº£n\s+\d+)',                         # Clause references  
            r'(ChÆ°Æ¡ng\s+[IVX]+)',                     # Chapter references
            r'(Má»¥c\s+\d+)',                           # Section references
            r'(Nghá»‹ Ä‘á»‹nh\s+\d+/\d+/NÄ-CP)',          # Decree references
            r'(ThÃ´ng tÆ°\s+\d+/\d+/TT-[A-Z]+)',       # Circular references
        ]
        
        for pattern in patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                references.append(match.group(1))
        
        return references
    
    def _extract_document_references(self, text: str) -> List[Dict[str, str]]:
        """Extract legal document references as entities"""
        entities = []
        
        # Law references
        law_pattern = r'(Luáº­t|Bá»™ luáº­t)\s+([^,.\n]+)'
        matches = re.finditer(law_pattern, text, re.IGNORECASE)
        for match in matches:
            entities.append({
                'type': 'legal_document',
                'subtype': 'law',
                'value': match.group(0),
                'law_type': match.group(1),
                'law_name': match.group(2)
            })
        
        return entities
    
    def _extract_monetary_amounts(self, text: str) -> List[Dict[str, str]]:
        """Extract monetary amounts"""
        entities = []
        
        # Vietnamese monetary patterns
        patterns = [
            r'(\d+(?:\.\d+)?)\s*(Ä‘á»“ng|VND)',
            r'(\d+(?:\.\d+)?)\s*(triá»‡u|tá»·)\s*(Ä‘á»“ng|VND)?',
            r'(\d+(?:,\d+)*)\s*(Ä‘á»“ng|VND)',
        ]
        
        for pattern in patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                entities.append({
                    'type': 'monetary_amount',
                    'value': match.group(0),
                    'amount': match.group(1),
                    'currency': match.group(2)
                })
        
        return entities
    
    def _extract_dates(self, text: str) -> List[Dict[str, str]]:
        """Extract dates in Vietnamese format"""
        entities = []
        
        # Vietnamese date patterns
        patterns = [
            r'(\d{1,2})/(\d{1,2})/(\d{4})',                    # dd/mm/yyyy
            r'(\d{1,2})-(\d{1,2})-(\d{4})',                    # dd-mm-yyyy
            r'ngÃ y\s+(\d{1,2})\s+thÃ¡ng\s+(\d{1,2})\s+nÄƒm\s+(\d{4})',  # Vietnamese format
            r'(\d{4})-(\d{1,2})-(\d{1,2})',                    # yyyy-mm-dd
        ]
        
        for pattern in patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                entities.append({
                    'type': 'date',
                    'value': match.group(0),
                    'raw_match': match.groups()
                })
        
        return entities
    
    def _extract_names(self, text: str) -> List[Dict[str, str]]:
        """Extract person and organization names"""
        entities = []
        
        # Vietnamese name patterns (basic)
        # This is a simplified version - real implementation would use NER
        patterns = [
            r'(Ã´ng|bÃ |anh|chá»‹)\s+([A-ZÃ€Ãáº áº¢ÃƒÃ‚áº¦áº¤áº¬áº¨áºªÄ‚áº°áº®áº¶áº²áº´ÃˆÃ‰áº¸áººáº¼ÃŠá»€áº¾á»†á»‚á»„ÃŒÃá»Šá»ˆÄ¨Ã’Ã“á»Œá»Ã•Ã”á»’á»á»˜á»”á»–Æ á»œá»šá»¢á»á» Ã™Ãšá»¤á»¦Å¨Æ¯á»ªá»¨á»°á»¬á»®á»²Ãá»´á»¶á»¸Ä][a-zÃ Ã¡áº¡áº£Ã£Ã¢áº§áº¥áº­áº©áº«Äƒáº±áº¯áº·áº³áºµÃ¨Ã©áº¹áº»áº½Ãªá»áº¿á»‡á»ƒá»…Ã¬Ã­á»‹á»‰Ä©Ã²Ã³á»á»ÃµÃ´á»“á»‘á»™á»•á»—Æ¡á»á»›á»£á»Ÿá»¡Ã¹Ãºá»¥á»§Å©Æ°á»«á»©á»±á»­á»¯á»³Ã½á»µá»·á»¹Ä‘]*(?:\s+[A-ZÃ€Ãáº áº¢ÃƒÃ‚áº¦áº¤áº¬áº¨áºªÄ‚áº°áº®áº¶áº²áº´ÃˆÃ‰áº¸áººáº¼ÃŠá»€áº¾á»†á»‚á»„ÃŒÃá»Šá»ˆÄ¨Ã’Ã“á»Œá»Ã•Ã”á»’á»á»˜á»”á»–Æ á»œá»šá»¢á»á» Ã™Ãšá»¤á»¦Å¨Æ¯á»ªá»¨á»°á»¬á»®á»²Ãá»´á»¶á»¸Ä][a-zÃ Ã¡áº¡áº£Ã£Ã¢áº§áº¥áº­áº©áº«Äƒáº±áº¯áº·áº³áºµÃ¨Ã©áº¹áº»áº½Ãªá»áº¿á»‡á»ƒá»…Ã¬Ã­á»‹á»‰Ä©Ã²Ã³á»á»ÃµÃ´á»“á»‘á»™á»•á»—Æ¡á»á»›á»£á»Ÿá»¡Ã¹Ãºá»¥á»§Å©Æ°á»«á»©á»±á»­á»¯á»³Ã½á»µá»·á»¹Ä‘]*)*)',
            r'(CÃ´ng ty|Doanh nghiá»‡p|Táº­p Ä‘oÃ n|NgÃ¢n hÃ ng)\s+([A-ZÃ€Ãáº áº¢ÃƒÃ‚áº¦áº¤áº¬áº¨áºªÄ‚áº°áº®áº¶áº²áº´ÃˆÃ‰áº¸áººáº¼ÃŠá»€áº¾á»†á»‚á»„ÃŒÃá»Šá»ˆÄ¨Ã’Ã“á»Œá»Ã•Ã”á»’á»á»˜á»”á»–Æ á»œá»šá»¢á»á» Ã™Ãšá»¤á»¦Å¨Æ¯á»ªá»¨á»°á»¬á»®á»²Ãá»´á»¶á»¸Ä][^,.\n]*)',
        ]
        
        for pattern in patterns:
            matches = re.finditer(pattern, text)
            for match in matches:
                if 'Ã´ng|bÃ |anh|chá»‹' in pattern:
                    entities.append({
                        'type': 'person',
                        'value': match.group(0),
                        'title': match.group(1),
                        'name': match.group(2)
                    })
                else:
                    entities.append({
                        'type': 'organization',
                        'value': match.group(0),
                        'org_type': match.group(1),
                        'org_name': match.group(2)
                    })
        
        return entities
    
    def _extract_procedures(self, text: str) -> List[Dict[str, str]]:
        """Extract legal procedures and processes"""
        entities = []
        
        # Legal procedure patterns
        procedures = [
            'khá»Ÿi kiá»‡n', 'tá»‘ tá»¥ng', 'phÃºc tháº©m', 'giÃ¡m Ä‘á»‘c tháº©m',
            'hÃ²a giáº£i', 'trá»ng tÃ i', 'thi hÃ nh Ã¡n', 'cÆ°á»¡ng cháº¿',
            'khÃ¡ng cÃ¡o', 'khÃ¡ng nghá»‹', 'táº¡m giam', 'táº¡m giá»¯',
            'Ä‘iá»u tra', 'truy tá»‘', 'xÃ©t xá»­', 'tuyÃªn Ã¡n'
        ]
        
        for procedure in procedures:
            pattern = r'\b' + re.escape(procedure) + r'\b'
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                entities.append({
                    'type': 'legal_procedure',
                    'value': match.group(0),
                    'procedure_type': procedure
                })
        
        return entities
    
    def _get_context_around_keyword(self, text: str, keyword: str, window: int = 50) -> str:
        """Get context around a keyword"""
        # TODO: Implement context extraction
        return ""
    
    def _normalize_date(self, date_str: str) -> str:
        """Normalize Vietnamese date to standard format"""
        # TODO: Implement date normalization
        return date_str
    
    def _calculate_language_confidence(self, text: str) -> float:
        """Calculate confidence that text is Vietnamese"""
        # Simple heuristic based on Vietnamese characters
        vietnamese_chars = 'Ã Ã¡áº¡áº£Ã£Ã¢áº§áº¥áº­áº©áº«Äƒáº±áº¯áº·áº³áºµÃ¨Ã©áº¹áº»áº½Ãªá»áº¿á»‡á»ƒá»…Ã¬Ã­á»‹á»‰Ä©Ã²Ã³á»á»ÃµÃ´á»“á»‘á»™á»•á»—Æ¡á»á»›á»£á»Ÿá»¡Ã¹Ãºá»¥á»§Å©Æ°á»«á»©á»±á»­á»¯á»³Ã½á»µá»·á»¹Ä‘'
        
        total_chars = len([c for c in text.lower() if c.isalpha()])
        if total_chars == 0:
            return 0.0
        
        vietnamese_char_count = len([c for c in text.lower() if c in vietnamese_chars])
        confidence = vietnamese_char_count / total_chars
        
        # Boost confidence if common Vietnamese words are found
        common_vn_words = ['vÃ ', 'cá»§a', 'trong', 'vá»›i', 'vá»', 'cho', 'tá»«', 'theo', 'nhÆ°']
        word_boost = sum(1 for word in common_vn_words if word in text.lower()) * 0.05
        
        return min(1.0, confidence + word_boost)
    
    def get_legal_domain(self, text: str) -> str:
        """Determine legal domain from Vietnamese text"""
        text_lower = text.lower()
        
        # Legal domain keywords
        domain_keywords = {
            'dan_su': ['dÃ¢n sá»±', 'há»£p Ä‘á»“ng', 'tÃ i sáº£n', 'quyá»n sá»Ÿ há»¯u', 'bá»“i thÆ°á»ng', 'thá»«a káº¿'],
            'hinh_su': ['hÃ¬nh sá»±', 'tá»™i pháº¡m', 'hÃ¬nh pháº¡t', 'Ã¡n tÃ¹', 'vi pháº¡m phÃ¡p luáº­t'],
            'lao_dong': ['lao Ä‘á»™ng', 'ngÆ°á»i lao Ä‘á»™ng', 'há»£p Ä‘á»“ng lao Ä‘á»™ng', 'báº£o hiá»ƒm xÃ£ há»™i', 'thá»i gian lÃ m viá»‡c'],
            'thuong_mai': ['thÆ°Æ¡ng máº¡i', 'kinh doanh', 'cÃ´ng ty', 'doanh nghiá»‡p', 'Ä‘Äƒng kÃ½ kinh doanh'],
            'hanh_chinh': ['hÃ nh chÃ­nh', 'thá»§ tá»¥c', 'cáº¥p phÃ©p', 'Ä‘Äƒng kÃ½', 'giáº¥y phÃ©p']
        }
        
        domain_scores = {}
        for domain, keywords in domain_keywords.items():
            score = sum(1 for keyword in keywords if keyword in text_lower)
            if score > 0:
                domain_scores[domain] = score
        
        if domain_scores:
            return max(domain_scores, key=domain_scores.get)
        
        return 'general'
    
    def extract_legal_citations(self, text: str) -> List[Dict[str, str]]:
        """Extract Vietnamese legal citations"""
        citations = []
        
        # Legal citation patterns
        patterns = [
            # Law citations
            r'(Luáº­t|Bá»™ luáº­t)\s+([^,.\n\d]+)\s+(\d{4})',
            # Article citations  
            r'Äiá»u\s+(\d+)\s+(Luáº­t|Bá»™ luáº­t)\s+([^,.\n\d]+)\s+(\d{4})',
            # Decree citations
            r'Nghá»‹ Ä‘á»‹nh\s+(\d+)/(\d{4})/NÄ-CP',
            # Circular citations
            r'ThÃ´ng tÆ°\s+(\d+)/(\d{4})/TT-([A-Z]+)',
        ]
        
        for pattern in patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                citation = {
                    'type': 'legal_citation',
                    'full_text': match.group(0),
                    'groups': match.groups()
                }
                
                # Parse specific citation types
                if 'Luáº­t' in match.group(0) or 'Bá»™ luáº­t' in match.group(0):
                    citation['citation_type'] = 'law'
                elif 'Nghá»‹ Ä‘á»‹nh' in match.group(0):
                    citation['citation_type'] = 'decree'
                elif 'ThÃ´ng tÆ°' in match.group(0):
                    citation['citation_type'] = 'circular'
                elif 'Äiá»u' in match.group(0):
                    citation['citation_type'] = 'article'
                
                citations.append(citation)
        
        return citations
        
        return min(vietnamese_count / total_chars * 2, 1.0)  # Scale up for better sensitivity
    
    def _load_legal_terms(self) -> List[str]:
        """Load Vietnamese legal terms dictionary"""
        # TODO: Load from external file or database
        return [
            'Bá»™ luáº­t DÃ¢n sá»±', 'Bá»™ luáº­t HÃ¬nh sá»±', 'Bá»™ luáº­t Lao Ä‘á»™ng',
            'Hiáº¿n phÃ¡p', 'Luáº­t ThÆ°Æ¡ng máº¡i', 'Luáº­t HÃ nh chÃ­nh',
            'há»£p Ä‘á»“ng', 'tÃ i sáº£n', 'quyá»n sá»Ÿ há»¯u', 'nghÄ©a vá»¥',
            'vi pháº¡m', 'trÃ¡ch nhiá»‡m', 'bá»“i thÆ°á»ng', 'tá»™i pháº¡m',
            'hÃ¬nh pháº¡t', 'an toÃ n lao Ä‘á»™ng', 'báº£o hiá»ƒm xÃ£ há»™i',
            'thá»§ tá»¥c hÃ nh chÃ­nh', 'cáº¥p phÃ©p', 'Ä‘Äƒng kÃ½ kinh doanh'
        ]
    
    def _load_vietnamese_stopwords(self) -> Set[str]:
        """Load Vietnamese stopwords"""
        # TODO: Load comprehensive stopwords list
        return {
            'lÃ ', 'cá»§a', 'vÃ ', 'cÃ³', 'Ä‘Æ°á»£c', 'trong', 'vá»›i', 'cho',
            'vá»', 'tá»«', 'theo', 'nhÆ°', 'Ä‘á»ƒ', 'khi', 'náº¿u', 'mÃ ',
            'cÃ¡c', 'nhá»¯ng', 'nÃ y', 'Ä‘Ã³', 'thÃ¬', 'sáº½', 'Ä‘Ã£', 'Ä‘ang'
        }
    
    def _load_legal_abbreviations(self) -> Dict[str, str]:
        """Load Vietnamese legal abbreviations"""
        return {
            'NÄ-CP': 'Nghá»‹ Ä‘á»‹nh cá»§a ChÃ­nh phá»§',
            'TT': 'ThÃ´ng tÆ°',
            'QÄ': 'Quyáº¿t Ä‘á»‹nh',
            'CV': 'CÃ´ng vÄƒn',
            'TB': 'ThÃ´ng bÃ¡o',
            'BLHS': 'Bá»™ luáº­t HÃ¬nh sá»±',
            'BLDS': 'Bá»™ luáº­t DÃ¢n sá»±',
            'BLLD': 'Bá»™ luáº­t Lao Ä‘á»™ng'
        }

class VietnameseQueryPreprocessor:
    """Preprocessor specifically for Vietnamese legal queries"""
    
    def __init__(self):
        self.text_processor = VietnameseTextProcessor()
    
    def preprocess_query(self, query: str) -> Dict[str, Any]:
        """Preprocess Vietnamese legal query for better search"""
        try:
            # Process text
            analysis = self.text_processor.process_legal_text(query)
            
            # Extract intent
            intent = self._extract_query_intent(query)
            
            # Generate search keywords
            search_keywords = self._generate_search_keywords(analysis)
            
            # Extract constraints
            constraints = self._extract_query_constraints(analysis)
            
            return {
                'original_query': query,
                'normalized_query': analysis.normalized_text,
                'intent': intent,
                'search_keywords': search_keywords,
                'legal_terms': analysis.legal_terms,
                'entities': analysis.entities,
                'constraints': constraints,
                'language_confidence': analysis.language_confidence
            }
            
        except Exception as e:
            logging.error(f"Query preprocessing failed: {e}")
            return {
                'original_query': query,
                'normalized_query': query,
                'intent': 'unknown',
                'search_keywords': query.split(),
                'legal_terms': [],
                'entities': [],
                'constraints': {},
                'language_confidence': 0.5
            }
    
    def _extract_query_intent(self, query: str) -> str:
        """Extract intent from Vietnamese legal query"""
        query_lower = query.lower()
        
        if any(word in query_lower for word in ['cÃ¡ch', 'lÃ m tháº¿ nÃ o', 'thá»§ tá»¥c']):
            return 'procedure_inquiry'
        elif any(word in query_lower for word in ['cÃ³ Ä‘Æ°á»£c', 'cÃ³ thá»ƒ', 'quyá»n']):
            return 'rights_inquiry'
        elif any(word in query_lower for word in ['pháº£i', 'báº¯t buá»™c', 'nghÄ©a vá»¥']):
            return 'obligation_inquiry'
        elif any(word in query_lower for word in ['vi pháº¡m', 'sai', 'lá»—i']):
            return 'violation_inquiry'
        else:
            return 'information_inquiry'
    
    def _generate_search_keywords(self, analysis: VietnameseTextAnalysis) -> List[str]:
        """Generate optimized search keywords"""
        keywords = []
        
        # Add legal terms (highest priority)
        keywords.extend(analysis.legal_terms)
        
        # Add important tokens
        important_tokens = [token for token in analysis.tokens if len(token) > 3]
        keywords.extend(important_tokens[:10])  # Limit to avoid noise
        
        # Add entity values
        for entity in analysis.entities:
            if entity.get('type') in ['legal_document', 'legal_procedure']:
                keywords.append(entity['value'])
        
        return list(set(keywords))  # Remove duplicates
    
    def _extract_query_constraints(self, analysis: VietnameseTextAnalysis) -> Dict[str, Any]:
        """Extract constraints from query"""
        constraints = {}
        
        # Date constraints
        date_entities = [e for e in analysis.entities if e['type'] == 'date']
        if date_entities:
            constraints['date_range'] = date_entities
        
        # Amount constraints
        amount_entities = [e for e in analysis.entities if e['type'] == 'monetary_amount']
        if amount_entities:
            constraints['amount_range'] = amount_entities
        
        # Legal domain constraints
        domain_keywords = {
            'dÃ¢n sá»±': 'dan_su',
            'hÃ¬nh sá»±': 'hinh_su', 
            'lao Ä‘á»™ng': 'lao_dong',
            'thÆ°Æ¡ng máº¡i': 'thuong_mai'
        }
        
        for keyword, domain in domain_keywords.items():
            if keyword in analysis.normalized_text.lower():
                constraints['legal_domain'] = domain
                break
        
        return constraints


# Utility functions for external use
def process_vietnamese_legal_text(text: str) -> VietnameseTextAnalysis:
    """Quick function to process Vietnamese legal text"""
    processor = VietnameseTextProcessor()
    return processor.process_legal_text(text)

def preprocess_vietnamese_query(query: str) -> Dict[str, Any]:
    """Quick function to preprocess Vietnamese legal query"""
    preprocessor = VietnameseQueryPreprocessor()
    return preprocessor.preprocess_query(query)

def extract_legal_terms(text: str) -> List[str]:
    """Quick function to extract legal terms from text"""
    processor = VietnameseTextProcessor()
    return processor.extract_legal_terms(text)

def get_legal_domain(text: str) -> str:
    """Quick function to get legal domain from text"""
    processor = VietnameseTextProcessor()
    return processor.get_legal_domain(text)

def extract_legal_citations(text: str) -> List[Dict[str, str]]:
    """Quick function to extract legal citations"""
    processor = VietnameseTextProcessor()
    return processor.extract_legal_citations(text)

def normalize_vietnamese_text(text: str) -> str:
    """Quick function to normalize Vietnamese text"""
    processor = VietnameseTextProcessor()
    return processor.normalize_vietnamese_text(text)

# Test function
def test_vietnamese_text_processing():
    """Test Vietnamese text processing functions"""
    
    # Test texts
    test_texts = [
        "Quyá»n dÃ¢n sá»± cá»§a cÃ´ng dÃ¢n Ä‘Æ°á»£c báº£o vá»‡ nhÆ° tháº¿ nÃ o?",
        "Äiá»u 15 Bá»™ luáº­t DÃ¢n sá»± 2015 quy Ä‘á»‹nh gÃ¬ vá» quyá»n sá»Ÿ há»¯u?",
        "Thá»i gian lÃ m viá»‡c theo Bá»™ luáº­t Lao Ä‘á»™ng 2019 lÃ  bao lÃ¢u?",
        "CÃ´ng ty tÃ´i yÃªu cáº§u lÃ m viá»‡c 10 giá»/ngÃ y cÃ³ vi pháº¡m khÃ´ng?"
    ]
    
    print("ğŸ‡»ğŸ‡³ VIETNAMESE TEXT PROCESSING TEST")
    print("=" * 60)
    
    for i, text in enumerate(test_texts, 1):
        print(f"\nğŸ“‹ TEST {i}: {text}")
        print("-" * 40)
        
        # Process text
        analysis = process_vietnamese_legal_text(text)
        print(f"âœ… Normalized: {analysis.normalized_text}")
        print(f"ğŸ” Legal terms: {analysis.legal_terms}")
        print(f"ğŸ·ï¸ Entities: {len(analysis.entities)} found")
        print(f"ğŸŒ Language confidence: {analysis.language_confidence:.2f}")
        
        # Preprocess query
        query_info = preprocess_vietnamese_query(text)
        print(f"ğŸ¯ Intent: {query_info['intent']}")
        print(f"ğŸ”‘ Keywords: {query_info['search_keywords'][:5]}")  # Show first 5
        print(f"ğŸ“š Domain: {get_legal_domain(text)}")
        
        # Extract citations
        citations = extract_legal_citations(text)
        if citations:
            print(f"ğŸ“– Citations: {len(citations)} found")
            for citation in citations:
                print(f"   - {citation['full_text']} ({citation['citation_type']})")
    
    print(f"\n{'='*60}")
    print("ğŸ‰ Vietnamese text processing test completed!")

if __name__ == "__main__":
    test_vietnamese_text_processing()
